{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking it Down: Gradient Descent\n",
    "\n",
    "In this post, we will: \n",
    "- Define gradient descent\n",
    "- Explore the calculus behind gradient descent for a univariate function\n",
    "- Visualize gradient descent of multivariate functions\n",
    "- Contextualize gradient descent with logistic regression\n",
    "\n",
    "**Outline**\n",
    "1. [What is Gradient Descent?](#1-what-is-logistic-regression)\n",
    "2. [Breaking Down Gradient Descent](#2-breaking-down-gradient-descent)\n",
    "    1. [Computing the gradient](#21-computing-the-gradient)\n",
    "    2. [Descending the gradient](#22-descending-the-gradient)\n",
    "3. [Descending multivariate functions](#3-descending-the-gradient-of-multivariate-functions)\n",
    "4. [Conclusion: Contextualizing Gradient Descent](#4-conclusion-contextualizing-gradient-descent)\n",
    "5. [Resources](#5-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Gradient Descent?\n",
    "Gradient descent is an optimization algorithm that is used to improve the performance of deep/machine learning models. Over a repeated series of training steps, gradient descent serves to identify optimal parameters to minimize the cost of a model. In the next section, we're going to step down from this satellite-view description and look more closely at what gradient descent actually is.\n",
    "\n",
    "<p align=\"center\">\n",
    "<video controls src=\"media/himmelblau_path7.mov\" width=\"60%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Breaking Down Gradient Descent\n",
    "To gain an intuitive understanding of gradient descent, let's first ignore machine and deep learning. To start, let's instead work with a simple function:\n",
    "\n",
    "$$ f(x) = x^2 $$\n",
    "\n",
    "The goal in gradient descent is to find the *minima* of a function, or the lowest possible output value of the function. In other words, given some function $f(x)$, how can we find the value of $x$ such that the output of $f(x)$ approaches $0$? For our simple example function, the obvious answer is $x = 0$.\n",
    "\n",
    "The important part of this problem is: if we initialize $x$ to some random number, say $x = 1.8$, is there some way to automatically update $x$ so that it eventually produces the *minimal* output of the function? This is essentially the goal in machine/deep learning gradient descent. We want to *automatically* find best weights in the model that will produce the *mimimal* output from our cost function.\n",
    "\n",
    "We can automatically find (or come close to) these minima with gradient descent in a two step process. \n",
    "\n",
    "1. First, we need to find the *gradient* (or *slope*) of the function at the point where our input parameter $x$ sits. \n",
    "2. Then, we need to *update* our input parameter $x$ by telling it to take a step *down* the gradient.\n",
    "\n",
    "This process is then repeated over and over until the output of our function stabilizes at a minima or it reaches a defined tolerance level.\n",
    "\n",
    "<p align=\"center\">\n",
    "<video controls src=\"media/x**2_descent.mov\" width=\"60%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Computing the gradient\n",
    "To find the slop (or *gradient*) of the function $f(x)$ at any value of $x$, we can differentiate* the function. Differentiating the simple example function is simple with the power rule $ \\frac{d}{dx}x^n = nx^{n-1}$, providing us with: $ f'(x) = 2x $.\n",
    "\n",
    "Using our starting point $x = 1.8$, we find our starting gradient to be $dx = 3.6$.\n",
    "\n",
    "Below let's write a simple function in python to automatically differentiate this function for us.\n",
    "\n",
    "###### *I'd strongly recommend checking out [3Blue1Brown's video][3b1b] to intuitively understand differentiation. The differentiation of this sample function from first principals can be seen [here][socratic].\n",
    "\n",
    "[3b1b]: https://www.youtube.com/watch?v=9vKqVkMQHKk&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=2&t=2s\n",
    "[socratic]: https://socratic.org/questions/how-you-you-find-the-derivative-f-x-x-2-using-first-principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at x = 1.8: dx = 3.6\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(x: float) -> float:\n",
    "    \"\"\"Compute the gradient of an input to the function f(x) = x**2.\n",
    "\n",
    "    Args:\n",
    "        x (float)\n",
    "\n",
    "    Returns:\n",
    "        float: dx\n",
    "    \"\"\"\n",
    "    dx = 2 * x\n",
    "    return dx\n",
    "\n",
    "x = 1.8\n",
    "dx = compute_gradient(x)\n",
    "print(f\"Gradient at x = 1.8: dx = {dx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Descending the gradient\n",
    "Once we find the gradient of the starting point, we want to update our input variable in such a way that will move it *down* this gradient. Again, we want to move *down* the gradient so that the output of our function will be minimized.\n",
    "\n",
    "To do this, we can simply subtract the gradient from the input variable. But if you've looked closely enough, you'd see that subtracting the entire gradient from the input variable $x$ would cause it to infinitely bounce back and forth from *1.8* to *-1.8*, never allowing it to approach *0*.\n",
    "\n",
    "Instead, we can define a *Learning_Rate = 0*. We'll use this learning rate to scale the gradient prior to subtracting it from our input variable. Large learning rates produce large jumps along the function, and small learning rates lead to small steps along the function.\n",
    "\n",
    "Lastly, we'll eventually have to stop the gradient descent, otherwise it would continue endlessly as it approaches 0. For this example, we'll simply stop the descent once the gradient of $x$, $dx < 0.01$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function minimum found in 27 iterations. X = 0.00\n"
     ]
    }
   ],
   "source": [
    "def descend_gradient(x: float, learning_rate: float = 0.1) -> float:\n",
    "    \"\"\"Descends gradient of a point on the input function f(x) = x**2.\n",
    "\n",
    "    Args:\n",
    "        x (float)\n",
    "        learning_rate (float): The rate by which the input variable is updated.\n",
    "        Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        float\n",
    "    \"\"\"\n",
    "    dx = compute_gradient(x)\n",
    "    x -= dx * learning_rate  # step the input variable 'down' the gradient\n",
    "    return x\n",
    "\n",
    "x = 1.8\n",
    "iterations = 0\n",
    "tolerance = 0.01\n",
    "while compute_gradient(x) > tolerance:\n",
    "    x = descend_gradient(x)\n",
    "    iterations += 1\n",
    "\n",
    "print(f\"Function minimum found in {iterations} iterations. X = {x:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the video above, our starting value of $x = 1.8$ was able to automatically be updated to $x = 0.0$ through the iterative process of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descending the gradient of multivariate functions\n",
    "Hopefully this univariate example provided some foundational insight into what gradient descent actually does. Let's look at it in the context of some multivariate functions now. \n",
    "\n",
    "Let's first visualize gradient descent of [Himmelblau's function][himmelblau].\n",
    "$$ f_{Himmelblau}(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$\n",
    "\n",
    "There will be a few key differences for the descent of multivariate functions.\n",
    "\n",
    "First, we will need to compute *partial* derivatives in order to update each variable. In the Himmelblau function above, the gradient of $x$ depends on $y$ (their sums are squared, requiring the chain rule). This means that the derivative of $x$ will contain $y$, and vice versa.\n",
    "\n",
    "Second, you may have noticed that in our first simple function that there was only one minima. In reality, there will be many unknown local minima in our models. This means that the starting point of our variables and the behavior of our descent function will change which minima the variables end in.\n",
    "\n",
    "To visualize the descent of this landscape, we're going to initialize our starting point as $x = 0.0$ and $y = -0.5 $. We can then watch the descent of each variable in it's own dimension, sliced by the position of the opposite variable. This 2D slices are overlaid above an $x, y$ plot of the function as the point descends to a minima.\n",
    "\n",
    "https://gfycat.com/ifr/MellowLegitimateEasteuropeanshepherd\n",
    "\n",
    "[himmelblau]:https://en.wikipedia.org/wiki/Himmelblau%27s_function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the descent of the same point in 3D using my [grad-descent-visualizer][gdv] package created with the help of [PyVista][pyvista].\n",
    "\n",
    "[gdv]: https://github.com/JacobBumgarner/grad-descent-visualizer\n",
    "[pyvista]: https://github.com/pyvista/pyvista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<video controls src=\"media/himmelblau_path6x.mov\" width=\"60%\">\n",
    "</p>\n",
    "\n",
    "From this visualization, we can see that the Himmelblau function has four global minima, and that the minima that the starting points land in depends on their original position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the descent of some more functions! We'll place a grid of points across each of these functions and watch how the points move as they descend whatever gradient they are sitting on.\n",
    "\n",
    "The [Sphere Function][sphere].\n",
    "\n",
    "<p align=\"center\">\n",
    "<video controls src=\"media/sphere.mov\" width=\"60%\">\n",
    "</p>\n",
    "\n",
    "The [Griewank Function][griewank].\n",
    "<p align=\"center\">\n",
    "<video controls src=\"media/griewank_functionx.mov\" width=\"60%\">\n",
    "</p>\n",
    "\n",
    "The [Six-Hump Camel Function][six-hump-camel]. Notice the many local minima of the function.\n",
    "<p align=\"center\">\n",
    "<video controls src=\"media/six_camel_path2x.mov\" width=\"60%\">\n",
    "</p>\n",
    "\n",
    "And lastly, the [Easom Function][easom]. Notice how many points sit still because they are initialized on a flat gradient.\n",
    "<p align=\"center\">\n",
    "<video controls src=\"media/easomx.mov\" width=\"60%\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[sphere]: https://www.sfu.ca/~ssurjano/spheref.html\n",
    "[griewank]: https://www.sfu.ca/~ssurjano/griewank.html\n",
    "[six-hump-camel]: https://www.sfu.ca/~ssurjano/camel6.html\n",
    "[easom]: https://www.sfu.ca/~ssurjano/easom.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion: Contextualizing Gradient Descent\n",
    "So far we've worked through gradient descent with a univariate function and have visualized descent of various multivariate functions. In reality, modern deep learning models have ***vastly*** more parameters than what we've worked three here. For example, Hugging Face's newest natural language processing model, Bloom, has *175 billion* parameters.\n",
    "\n",
    "This number of parameters is definitely intimidating, and the functions are certainly more complex than what we've examined in this post.\n",
    "\n",
    "It's important to realize that the *foundations* of what we've learned still apply. During each iteration of training, the gradient of each parameter is calculated. This gradient will then be subtracted from the parameter so that the parameter 'steps down' it's gradient, pushing it to produce a minimal output from the model's cost function.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Resources\n",
    "- [3Blue1Brown](https://www.youtube.com/c/3blue1brown)  \n",
    "    - [Gradient Descent](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "    - [Derivatives](https://www.youtube.com/watch?v=9vKqVkMQHKk&t=10s)\n",
    "- [Simon Fraser University: Test Functions for Optimization](https://www.sfu.ca/~ssurjano/optimization.html)\n",
    "- [PyVista](https://docs.pyvista.org)\n",
    "- [Michael Nielsen's Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "083c3123b4ad7f25f53c003e80272d3d1894a33e093a79f10823ee80b0414ebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
