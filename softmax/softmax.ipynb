{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "**Outline**\n",
    "1. [What is Softmax?](#1-what-is-softmax)\n",
    "2. [Getting Started](#2-getting-started)\n",
    "3. [Implementing softmax from scratch with NumPy](#3-implementing-softmax-from-scratch-with-numpy)\n",
    "4. [Implementing softmax with TensorFlow](#4-implementing-softmax-with-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Softmax?\n",
    "[Softmax][softmax] is an **activation function*** often used in multiclass classification models in deep learning. Softmax serves to convert a vector of $n$ real numbers into a probability distribution vector of $n$ numbers that sums to $1$. \n",
    "\n",
    "Softmax produces this output probabilty vector by first applying the standard exponential function to the input vector and then mean normalizing the exponentiated vector to produce a vector sum of $1$. The softmax equation is found below:\n",
    "\n",
    "$$\\textit{softmax}(x_{i}) = \\frac{e^{x_{i}}}{\\sum_{i=1}^{n}e^{x_{i}}}$$\n",
    "\n",
    "**Questions that I had when learning about softmax:**\n",
    "\n",
    ">**Q1:** Why does softmax exponentiate the input vector? <br><br>\n",
    ">**A1:** Softmax aims to produce a probabilty distribution vector that sums to 1. To achieve this, it must handle non-negative numbers. Moreover, softmax aims to apply greater probabilities to monotonically larger numbers. Exponentiating the input vector can resolve both of these needs.\n",
    "\n",
    ">**Q2:** Why does softmax use the standard exponential function instead of another base?<br><br>\n",
    ">**A2:** Finding the answer to this one was a bit more difficult for me. It seems that on some level, choosing $e$ was slightly arbitrary, as there are other forms of softmax functions<sup>[1][stackexchange],[2][arxiv]</sup>. However, it seems possible that the main reason for selecting $e$ is because it is so easily differentiable, as $\\frac{d}{dx}e^x = e^x$. [3Blue1Brown][3b1b] has a great explanation of this. \n",
    "\n",
    ">**Q3:** Why use softmax instead using sigmoid activations for each final unit and finding the largest sigmoid output?\n",
    ">**A3:** Hardmax is not differentiable, whereas softmax is. Moreover, hardmax results in information loss and can hinder training.\n",
    "\n",
    ">**Final note:** It was important for me to differentiate between 'multi-class' and 'multi-label' classifications when learning about softmax. Softmax should be used when there is only *one* correct output label. When more than one label is possible, sigmoid activations should be used for each final unit to determine whether the label is present, regardless of other labels.\n",
    "\n",
    "\n",
    "***Related** - Activation functions are non-linear functions that produce an output value for a unit. This output value represents the unit's level of activation, given some input value.\n",
    "\n",
    "[softmax]: https://en.wikipedia.org/wiki/Softmax_function\n",
    "[stackexchange]: https://stats.stackexchange.com/questions/296471/why-e-in-softmax\n",
    "[arxiv]: https://arxiv.org/abs/1511.05042\n",
    "[3b1b]: https://www.youtube.com/watch?v=m2MIpDrF7Es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting Started\n",
    "To place the softmax activation function into a realistic context, let's imagine we've trained a CNN that serves to recognize four types of animals and a classifies any other unknown image as 'other'.\n",
    "\n",
    "At the end of our model, we have five output units. These units provide their raw output values to the softmax layer, which converts the outputs into a probability distribution vector for the classification of our input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementing Softmax from Scratch with NumPy\n",
    "Now let's say that we've just run the following image of an elephant through our CNN, and the final five units produce the five output values below.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/Softmax Figure.png\" alt=\"example multi-class cnn with softmax output/\" width=\"75%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw_output = np.array([[3.8, 0.2, -0.6, 0.9, 0.3]])  # AKA logits in TF jargon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write a softmax function to convert this raw ouptut vector into a probability distribution vector. The function will be quite short!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output values: [0.88902982 0.0242916  0.01091492 0.04891728 0.02684637]\n"
     ]
    }
   ],
   "source": [
    "def softmax(input_vector: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply softmax to an input vector and return result.\n",
    "    \n",
    "    Parameters:\n",
    "        input_vector : np.ndarray\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    exp = np.exp(input_vector)\n",
    "    prob_vector = exp / np.sum(exp)\n",
    "    \n",
    "    return prob_vector\n",
    "\n",
    "output_probabilities = softmax(raw_output)\n",
    "\n",
    "print(f\"Softmax output values: {output_probabilities[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this mock example, our model is classifying the with an ~89% probability of containing an elephant. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementing softmax with TensorFlow\n",
    "As usual, it's better to tend to rely on well-established packages for most needs. We can apply softmax in a few different ways using TensorFlow. Below are two examples. We'll print the output value of the first example to compare it to our own function.\n",
    "\n",
    "1. It can be applied as an individual activation layer for our model.\n",
    "\n",
    "2. It can be applied as an activation function for a `tf.keras.layers.Dense` fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF softmax output values: [0.88902982 0.0242916  0.01091492 0.04891728 0.02684637]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# first let's convert the numpy array to a tensor\n",
    "raw_output_tensor = tf.convert_to_tensor(raw_output)\n",
    "\n",
    "# Example 1. Create a separate activation layer\n",
    "output = tf.keras.activations.softmax(raw_output_tensor)\n",
    "print(f\"TF softmax output values: {output_probabilities[0]}\")\n",
    "\n",
    "# Example 2. create a FC layer in a model with a softmax activation. \n",
    "#   We'll treat this layer as if it were the final layer of our example model.\n",
    "layer = tf.keras.layers.Dense(5, activation=tf.keras.activations.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to create an issue in the repository if you have an concerns or find any problems with this demonstration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "083c3123b4ad7f25f53c003e80272d3d1894a33e093a79f10823ee80b0414ebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
